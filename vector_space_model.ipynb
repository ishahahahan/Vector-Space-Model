{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ishan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\ishan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ishan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ishan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ishan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\ishan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\Ishan\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read documents from a directory\n",
    "def read_documents(directory):\n",
    "    documents = {}\n",
    "    try:\n",
    "        for filename in os.listdir(directory):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                tokenized = preprocess(text)\n",
    "                documents[filename] = tokenized\n",
    "    except Exception as e:\n",
    "        print('An error occurred:', e)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dictionary of all words in the documents\n",
    "def create_dictionary(documents):\n",
    "    dictionary = set()\n",
    "    for document in documents.values():\n",
    "        dictionary.update(document)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the term frequency\n",
    "\n",
    "def term_frequency(documents):\n",
    "    tf = defaultdict(lambda: defaultdict(int))\n",
    "    for filename, tokens in documents.items():\n",
    "        for token in tokens:\n",
    "            tf[filename][token] += 1\n",
    "    return tf\n",
    "\n",
    "# Weighted term frequency\n",
    "\n",
    "def weighted_term_frequency(tf):\n",
    "    return 1 + math.log10(tf) if tf > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_document_frequencies(posting_list):\n",
    "    # Implement document frequency calculation\n",
    "    document_frequencies = {}\n",
    "    for term, postings in posting_list.items():\n",
    "        document_frequencies[term] = len(postings)\n",
    "    return document_frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the inverse document frequency\n",
    "def postings_list(documents, unique_words):\n",
    "    # Initialize postings list as a defaultdict of lists\n",
    "    postings = defaultdict(list)\n",
    "    \n",
    "    # Compute term frequency for the documents\n",
    "    tf = term_frequency(documents)\n",
    "    \n",
    "    # Iterate over each unique word\n",
    "    for word in unique_words:\n",
    "        # Iterate over each document and its tokens\n",
    "        for filename, tokens in documents.items():\n",
    "            # Check if the word is in the document tokens\n",
    "            if word in tokens:\n",
    "                try:\n",
    "                    # Calculate weighted term frequency\n",
    "                    wt_tf = weighted_term_frequency(tf[filename][word])\n",
    "                except KeyError:\n",
    "                    # Handle case where term is not found in the document\n",
    "                    print(f\"Warning: Term '{word}' not found in document {filename}\")\n",
    "                    wt_tf = 0\n",
    "                # Append the filename and weighted term frequency to the postings list\n",
    "                postings[word].append((filename, wt_tf))\n",
    "    \n",
    "    return postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the document length\n",
    "\n",
    "def doc_length(tf):\n",
    "    doc_lengths = defaultdict(float)\n",
    "    for filename, terms in tf.items():\n",
    "        length = 0\n",
    "        for term, freq in terms.items():\n",
    "            length += (1 + math.log10(freq)) ** 2\n",
    "        doc_lengths[filename] = math.sqrt(length)\n",
    "    return doc_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity for the given document\n",
    "def cosine_similarity(query_wt, doc_wt, doc_len, doc_id):\n",
    "    similarity = {}\n",
    "\n",
    "    # Calculate dot product of query and document weights\n",
    "    dot_product = 0\n",
    "    for term in query_wt:\n",
    "        if term in doc_wt:\n",
    "            dot_product += query_wt[term] * doc_wt[term]\n",
    "    \n",
    "    # Calculate magnitude of the query vector\n",
    "    query_magnitude = 0\n",
    "    for weight in query_wt.values():\n",
    "        query_magnitude += weight ** 2\n",
    "    query_magnitude = math.sqrt(query_magnitude)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity[doc_id] = dot_product / (query_magnitude * doc_len[doc_id])\n",
    "    \n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(freq):\n",
    "    # Calculate term frequency using log normalization\n",
    "    return 1 + math.log10(freq) if freq > 0 else 0\n",
    "\n",
    "def calculate_idf(df, N):\n",
    "    # Calculate inverse document frequency\n",
    "    return math.log10(N / df)\n",
    "\n",
    "def rank_documents(documents, query, posting_list, document_frequencies, unique_words):\n",
    "    N = len(documents)  # Total number of documents\n",
    "    query_tokens = preprocess(query)  # Preprocess the query\n",
    "    unique_words_query = set(query_tokens)  # Unique words in the query\n",
    "    unique_words = unique_words.union(unique_words_query)  # Combine unique words from documents and query\n",
    "    query_vector = {}\n",
    "\n",
    "    # Calculate query tf-idf weights (ltc scheme)\n",
    "    for word in unique_words:\n",
    "        tf = query_tokens.count(word)  # Term frequency in the query\n",
    "        df = document_frequencies.get(word, 0)  # Document frequency of the term\n",
    "        if df > 0:\n",
    "            idf = calculate_idf(df, N)  # Inverse document frequency\n",
    "            query_vector[word] = calculate_tf(tf) * idf  # tf-idf weight for the query term\n",
    "\n",
    "    # Calculate document lengths (for cosine similarity)\n",
    "    doc_lengths = defaultdict(float)\n",
    "    for word, postings in posting_list.items():\n",
    "        for doc, log_tf in postings:\n",
    "            doc_lengths[doc] += log_tf ** 2\n",
    "    for doc in doc_lengths:\n",
    "        doc_lengths[doc] = math.sqrt(doc_lengths[doc])  # Finalize document lengths\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    similarities = {}\n",
    "    for doc_name in documents.keys():\n",
    "        doc_vector = {}\n",
    "        for word in unique_words:\n",
    "            posting = posting_list.get(word, [])\n",
    "            for doc, log_tf in posting:\n",
    "                if doc == doc_name:\n",
    "                    doc_vector[word] = log_tf  # Document vector for the term\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(query_vector, doc_vector, doc_lengths, doc_name)\n",
    "        similarities.update(similarity)\n",
    "\n",
    "    # Sort documents by similarity and return all ranked documents\n",
    "    ranked_docs = sorted(similarities.items(), key=lambda item: (-item[1], item[0]))\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked document by relevance to query 1:\n",
      "zomato.txt: 0.2163\n",
      "swiggy.txt: 0.1350\n",
      "messenger.txt: 0.0608\n",
      "instagram.txt: 0.0598\n",
      "Discord.txt: 0.0548\n",
      "bing.txt: 0.0524\n",
      "youtube.txt: 0.0494\n",
      "paypal.txt: 0.0481\n",
      "reddit.txt: 0.0454\n",
      "flipkart.txt: 0.0416\n",
      "\n",
      "\n",
      "Ranked document by relevance to query 2:\n",
      "shakespeare.txt: 0.1235\n",
      "levis.txt: 0.0260\n",
      "google.txt: 0.0216\n",
      "nike.txt: 0.0194\n",
      "zomato.txt: 0.0178\n",
      "Adobe.txt: 0.0167\n",
      "huawei.txt: 0.0145\n",
      "skype.txt: 0.0126\n",
      "blackberry.txt: 0.0119\n",
      "reliance.txt: 0.0111\n"
     ]
    }
   ],
   "source": [
    "def index_corpus(corpus_path):\n",
    "    documents = read_documents(corpus_path)\n",
    "    unique_words = create_dictionary(documents)\n",
    "    posting_list = postings_list(documents, unique_words)\n",
    "    document_frequencies = calculate_document_frequencies(posting_list)\n",
    "    \n",
    "    return documents, posting_list, document_frequencies, unique_words\n",
    "\n",
    "def search(query, documents, posting_list, document_frequencies, unique_words):\n",
    "    ranked_docs = rank_documents(documents, query, posting_list, document_frequencies, unique_words)\n",
    "    \n",
    "    # Format the output\n",
    "    formatted_results = []\n",
    "    for filename, score in ranked_docs[:10]:  # Get top 10 results\n",
    "        formatted_results.append(f\"{filename}: {score:.4f}\")\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "# Example usage\n",
    "corpus_path = 'Corpus'\n",
    "documents, posting_list, document_frequencies, unique_words = index_corpus(corpus_path)\n",
    "\n",
    "query1 = 'Developing your Zomato business account and profile is a great way to boost your restaurant\\'s online reputation'\n",
    "ranked_docs = search(query1, documents, posting_list, document_frequencies, unique_words)\n",
    "\n",
    "query2 = \"Warwickshire, came from an ancient family and was the heiress to some land\"\n",
    "ranked_docs2 = search(query2, documents, posting_list, document_frequencies, unique_words)\n",
    "\n",
    "# Print the results\n",
    "print(\"Ranked document by relevance to query 1:\")\n",
    "for result in ranked_docs:\n",
    "    print(result)\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Ranked document by relevance to query 2:\")\n",
    "for result in ranked_docs2:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
